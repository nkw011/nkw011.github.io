---
title: "[Huggingface ğŸ¤— Transformers Tutorial] 4. Fine-tune a pretrained model"
excerpt: "ğŸ¤— Transformersë¥¼ ì´ìš©í•˜ì—¬ pretrained modelì„ fine-tuningí•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê³  sentiment analysis(ê°ì • ë¶„ì„) taskë¥¼ ê°„ë‹¨í•˜ê²Œ ìˆ˜í–‰í•´ë´…ë‹ˆë‹¤."
toc: true
toc_sticky: true
categories:
    - nlp
tags:
    - huggingface
    - transformers
sidebar:
    nav: sidebarTotal
---

â€» ì´ ê¸€ì˜ ì›ë¬¸ì€ [ì´ ê³³](https://huggingface.co/docs/transformers/training#finetune-a-pretrained-model)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â€» ëª¨ë“  ê¸€ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©° ìƒˆë¡­ê²Œ êµ¬ì„±í•œ ë‚´ìš©ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ğŸ¤— Transformersì— ì˜¬ë¼ì™€ìˆëŠ” pretrained ëª¨ë¸ë“¤ì„ specific taskì— ë§ê²Œ trainí•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
* fine-tuning


```python
!pip install transformers
!pip install datasets
```

í™œìš©í•  ë°ì´í„°ì…‹ì€ [nsmc(naver sentiment movie corpus)](https://huggingface.co/datasets/nsmc)ì…ë‹ˆë‹¤.
* ì˜í™” ë¦¬ë·° ëŒ“ê¸€ì„ ì´ìš©í•´ ê°ì • ë¶„ë¥˜í•˜ëŠ” ëª©ì ìœ¼ë¡œ ì œì‘ëœ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
* 0(negative), 1(postiive)


```python
from datasets import load_dataset

nsmc = load_dataset('nsmc')
```


```python
nsmc
```




    DatasetDict({
        train: Dataset({
            features: ['id', 'document', 'label'],
            num_rows: 150000
        })
        test: Dataset({
            features: ['id', 'document', 'label'],
            num_rows: 50000
        })
    })



train setì€ 150000ê°œ, test setì€ 50000ê°œë¡œ ì´ë£¨ì–´ì ¸ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ë ¤ë©´ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— í•™ìŠµ ë°ì´í„° 2000ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„° 2000ê°œë¥¼ random samplingí•˜ê² ìŠµë‹ˆë‹¤.


```python
train_data = nsmc['train'].shuffle(seed=42).select(range(2000))
test_data = nsmc['test'].shuffle(seed=42).select(range(2000))
```


```python
train_data
```




    Dataset({
        features: ['id', 'document', 'label'],
        num_rows: 2000
    })




```python
test_data
```




    Dataset({
        features: ['id', 'document', 'label'],
        num_rows: 2000
    })



ì˜í™” ëŒ“ê¸€ ê°ì • ë¶„ë¥˜ ì‘ì—…ì„ ì‹¤ìŠµí•´ë³¼ ê²ƒì…ë‹ˆë‹¤. ë¶„ë¥˜ë¥¼ ìœ„í•œ modelê³¼ tokenizerë¥¼ loadí•©ë‹ˆë‹¤.
* ë¬¸ì¥ì´ ì–´ë–¤ ê°ì •ì— í•´ë‹¹í•˜ëŠ” ì§€ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— Sequence Classificationì„ ìœ„í•œ AutoClassì¸ AutoModelForSequenceClassificationì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
* modelê³¼ tokenizerë¡œ ['bert-base-multilingual-cased'](https://huggingface.co/bert-base-multilingual-cased)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.


```python
MODEL_NAME = 'bert-base-multilingual-cased'
```


```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

tokenizerê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.


```python
tokenizer.tokenize(train_data['document'][0])
```




    ['For',
     'Carl',
     '.',
     'ì¹¼',
     'ì„¸',
     '##ì´',
     '##ê±´',
     '##ìœ¼ë¡œ',
     'ì‹œ',
     '##ì‘',
     '##í•´ì„œ',
     'ì¹¼',
     'ì„¸',
     '##ì´',
     '##ê±´',
     '##ìœ¼ë¡œ',
     'ë',
     '##ë‚œ',
     '##ë‹¤',
     '.']



ë¨¼ì € dataì˜ documentë¥¼ ëª¨ë‘ encodingí•©ë‹ˆë‹¤.


```python
train_encoding = tokenizer(
    train_data['document'],
    return_tensors='pt',
    padding=True,
    truncation=True
)

test_encoding = tokenizer(
    test_data['document'],
    return_tensors='pt',
    padding=True,
    truncation=True
)
```


```python
len(train_encoding['input_ids']), len(test_encoding['input_ids'])
```




    (2000, 2000)



í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.

BERTë¥¼ ì´ìš©í•´ ì˜í™” ëŒ“ê¸€ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ í•  ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.
1. input_ids
2. token_type_ids
3. attention_mask
4. labels

1~3ë²ˆì€ tokenizerë¥¼ ì´ìš©í•´ ì´ë¯¸ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. labelsëŠ” random sampledëœ datasetì˜ labelì„ ì´ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

PyTorchì˜ Dataset í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ í•™ìŠµê³¼ ê²€ì¦ì„ ìœ„í•œ ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤.


```python
import torch
from torch.utils.data import Dataset


class NSMCDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encoding = encodings
        self.labels = labels

    def __getitem__(self, idx):
        data = {key: val[idx] for key, val in self.encoding.items()}
        data['labels'] = torch.tensor(self.labels[idx]).long()

        return data

    def __len__(self):
        return len(self.labels)
```


```python
train_set = NSMCDataset(train_encoding, train_data['label'])
test_set = NSMCDataset(test_encoding, test_data['label'])
```


```python
train_set[0]
```




    {'input_ids': tensor([  101, 11399, 12225,   119,  9788,  9435, 10739, 71439, 11467,  9485,
             38709, 70146,  9788,  9435, 10739, 71439, 11467,  8977, 33305, 11903,
               119,   102,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0]),
     'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),
     'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),
     'labels': tensor(1)}




```python
test_set[0]
```




    {'input_ids': tensor([  101, 14796, 27728, 10230,   106,   102,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
                 0,     0,     0,     0,     0]),
     'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0]),
     'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0]),
     'labels': tensor(0)}



## Trainer í´ë˜ìŠ¤ë¡œ í•™ìŠµí•˜ê¸°

ğŸ¤— TransformersëŠ” model í•™ìŠµì„ ìœ„í•´ TrainingArguments, Trainer í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
* TrainingArguments Trainerë¥¼ ìœ„í•œ Argument í´ë˜ìŠ¤ë¼ê³  ë³´ë©´ë©ë‹ˆë‹¤.

TrainingArguments, Trainerë¥¼ ì´ìš©í•˜ë©´ training option, logging, gradient accumulation, mixed precisionì„ ê°„ë‹¨í•˜ê²Œ ì„¤ì •í•´ í•™ìŠµ, í‰ê°€ë¥¼ ëª¨ë‘ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



```python
from transformers import TrainingArguments, Trainer
```

ë¨¼ì € Trainingì— í•„ìš”í•œ argumentë¥¼ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.
* ì•„ë˜ ì‚¬ìš©í•œ parameterë³´ë‹¤ ë‹¤ì–‘í•œ parameterê°€ ì¡´ì¬í•˜ë‹ˆ [TrainingArguments](https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/trainer#transformers.TrainingArguments)ë¥¼ ì°¸ê³ í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.


```python
training_args = TrainingArguments(
    output_dir = './outputs', # modelì´ ì €ì¥ë˜ëŠ” directory
    logging_dir = './logs', # logê°€ ì €ì¥ë˜ëŠ” directory
    num_train_epochs = 10, # training epoch ìˆ˜
    per_device_train_batch_size=32,  # train batch size
    per_device_eval_batch_size=32,   # eval batch size
    logging_steps = 50, # logging step, batchë‹¨ìœ„ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— epochìˆ˜ë¥¼ ê³±í•œ ì „ì²´ ë°ì´í„° í¬ê¸°ë¥¼ batchí¬ê¸°ë¡œ ë‚˜ëˆ„ë©´ ì´ step ê°¯ìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
    save_steps= 50, # 50 stepë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í•œë‹¤.
    save_total_limit=2 # 2ê°œ ëª¨ë¸ë§Œ ì €ì¥í•œë‹¤.
)
```

GPU í•™ìŠµì„ ìœ„í•´ deviceë¥¼ cudeë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

TrainingArgumentsë¥¼ ì´ìš©í•´ Trainerë¥¼ ë§Œë“­ë‹ˆë‹¤.


```python
device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')

device
```




    device(type='cuda')



Trainer í´ë˜ìŠ¤ëŠ” ë³„ë„ì˜ metricì„ ì œê³µí•´ì£¼ì§€ì•Šê¸° ë•Œë¬¸ì— ë³„ë„ì˜ í•¨ìˆ˜ë¥¼ í†µí•´ ê³„ì‚°ì„ ë”°ë¡œ í•´ì£¼ì–´ì•¼í•©ë‹ˆë‹¤.

accuracyì™€ f1 scoreë¥¼ ê³„ì‚°í•˜ê¸°ìœ„í•œ compute_metrics í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.
* í•´ë‹¹ í•¨ìˆ˜ëŠ” ì¸ìë¥¼ í†µí•´ EvalPrediction ê°ì²´ë¥¼ ë„˜ê²¨ ë°›ìŠµë‹ˆë‹¤.
* EvalPredictionì€ predictionsì™€ label_idsë¥¼ ê°€ì§‘ë‹ˆë‹¤.
    * predictions: modelì˜ ì˜ˆì¸¡ê°’
    * label_ids: label ê°’
* datasetsì—ì„œ ì œê³µí•˜ëŠ” load_metric()ì„ ì´ìš©í•´ accuracyì™€ f1 scoreë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.



```python
from datasets import load_metric

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)

    m1 = load_metric('accuracy')
    m2 = load_metric('f1')

    acc = m1.compute(predictions=preds, references=labels)['accuracy']
    f1 = m2.compute(predictions=preds, references=labels)['f1']

    return {'accuracy':acc, 'f1':f1}
```


```python
model.to(device)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_set, # í•™ìŠµ ì„¸íŠ¸
    eval_dataset=test_set, # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸
    compute_metrics=compute_metrics # metric ê³„ì‚° í•¨ìˆ˜
)
```

modelì„ í•™ìŠµí•©ë‹ˆë‹¤.


```python
trainer.train()
```

    ***** Running training *****
      Num examples = 2000
      Num Epochs = 10
      Instantaneous batch size per device = 32
      Total train batch size (w. parallel, distributed & accumulation) = 32
      Gradient Accumulation steps = 1
      Total optimization steps = 630





<table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>50</td>
      <td>0.697200</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.631900</td>
    </tr>
    <tr>
      <td>150</td>
      <td>0.509600</td>
    </tr>
    <tr>
      <td>200</td>
      <td>0.401600</td>
    </tr>
    <tr>
      <td>250</td>
      <td>0.314900</td>
    </tr>
    <tr>
      <td>300</td>
      <td>0.210400</td>
    </tr>
    <tr>
      <td>350</td>
      <td>0.155300</td>
    </tr>
    <tr>
      <td>400</td>
      <td>0.105700</td>
    </tr>
    <tr>
      <td>450</td>
      <td>0.101600</td>
    </tr>
    <tr>
      <td>500</td>
      <td>0.058900</td>
    </tr>
    <tr>
      <td>550</td>
      <td>0.046800</td>
    </tr>
    <tr>
      <td>600</td>
      <td>0.041600</td>
    </tr>
  </tbody>
</table>

    TrainOutput(global_step=630, training_loss=0.26070984389100754, metrics={'train_runtime': 507.216, 'train_samples_per_second': 39.431, 'train_steps_per_second': 1.242, 'total_flos': 1223055296400000.0, 'train_loss': 0.26070984389100754, 'epoch': 10.0})



modelì„ í‰ê°€í•©ë‹ˆë‹¤.


```python
trainer.evaluate()
```

    ***** Running Evaluation *****
      Num examples = 2000
      Batch size = 32
    {'eval_loss': 1.3575892448425293,
     'eval_accuracy': 0.762,
     'eval_f1': 0.7586206896551724,
     'eval_runtime': 16.4764,
     'eval_samples_per_second': 121.386,
     'eval_steps_per_second': 3.824}


ë°ì´í„° ìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì— accuracyê°€ ì•½ 76ì •ë„ ë‚˜ì˜¨ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## PyTorch Native ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê¸°

TrainingArgumentsì™€ Trainerë¥¼ ì‚¬ìš©í•˜ì§€ì•Šê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ë¨¼ì € DataLoaderë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. batch sizeëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ 32ì…ë‹ˆë‹¤.


```python
from torch.utils.data import DataLoader

train_loader = DataLoader(train_set, batch_size=32)
test_loader = DataLoader(test_set, batch_size=32)
```

ë‹¤ìŒìœ¼ë¡œëŠ” train í•¨ìˆ˜ë¥¼ êµ¬ì„±í•  ê²ƒì…ë‹ˆë‹¤.

train í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ modelì˜ outputì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì•¼í•©ë‹ˆë‹¤.

dummy ë°ì´í„°ë¥¼ ì´ìš©í•´ modelì˜ outputì´ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ” ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.


```python
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
```


```python
dummy = tokenizer(train_data['document'][0], return_tensors='pt')

model(**dummy)
```




    SequenceClassifierOutput(loss=None, logits=tensor([[-0.0045,  0.1503]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)



modelì˜ outputì—ì„œ ì£¼ëª©í•  ê²ƒì€ lossì™€ logitsì…ë‹ˆë‹¤. logitsì€ modelì˜ ì˜ˆì¸¡ê°’ì„ ê°€ë¦¬í‚µë‹ˆë‹¤. lossëŠ” ë§ê·¸ëŒ€ë¡œ loss ê°’ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
* labelì´ ì—†ê¸° ë•Œë¬¸ì— lossê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

ì´ 2ê°€ì§€ ì •ë³´ë¥¼ ì´ìš©í•´ train í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ê² ìŠµë‹ˆë‹¤.

train í•¨ìˆ˜ì—ëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ accuracyì™€ f1 scoreë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒë„ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤.


```python
from tqdm.notebook import tqdm
from datasets import load_metric

def train(epoch, model, dataloader, optimizer, device):
    model.to(device)

    m1 = load_metric('accuracy')
    m2 = load_metric('f1')

    for e in range(1, epoch+1):
        total_loss = 0.
        preds = []
        labels = []
        progress_bar = tqdm(dataloader, desc=f'TRAIN - EPOCH {e} |')
        for data in progress_bar:
            data = {k:v.to(device) for k, v in data.items()}
            output = model(**data)
            current_loss = output.loss

            total_loss += current_loss
            preds += list(output.logits.argmax(-1))
            labels += list(data['labels'].detach().cpu().numpy())

            optimizer.zero_grad()
            current_loss.backward()
            optimizer.step()

            progress_bar.set_description(f'TRAIN - EPOCH {e} | current-loss: {current_loss:.4f}')
        
        acc = m1.compute(predictions=preds, references=labels)['accuracy']
        f1 = m2.compute(predictions=preds, references=labels)['f1']
        avg = total_loss / len(dataloader)

        print('='*64)
        print(f"TRAIN - EPOCH {e} | LOSS: {avg:.4f} ACC: {acc:.4f} F1: {f1:.4f}")
        print('='*64)

```

evaluate í•¨ìˆ˜ë„ trainí•¨ìˆ˜ì™€ ë¹„ìŠ·í•˜ê²Œ êµ¬ì„±í•˜ì§€ë§Œ epochì´ ì—†ê³  backward ê³¼ì •ì´ ì—†ìŠµë‹ˆë‹¤.


```python
def evaluate(model, dataloader, device):
    model.to(device)

    m1 = load_metric('accuracy')
    m2 = load_metric('f1')

    total_loss = 0.
    preds = []
    labels = []
    progress_bar = tqdm(dataloader, desc=f'EVAL |')
    for data in progress_bar:
        data = {k:v.to(device) for k, v in data.items()}

        with torch.no_grad():
            output = model(**data)
    
        current_loss = output.loss

        total_loss += current_loss
        preds += list(output.logits.argmax(-1))
        labels += list(data['labels'].detach().cpu().numpy())

        progress_bar.set_description(f'EVAL | current-loss: {current_loss:.4f}')
    
    acc = m1.compute(predictions=preds, references=labels)['accuracy']
    f1 = m2.compute(predictions=preds, references=labels)['f1']
    avg = total_loss / len(dataloader)

    print('='*64)
    print(f"EVAL | LOSS: {avg:.4f} ACC: {acc:.4f} F1: {f1:.4f}")
    print('='*64)
```

í•¨ìˆ˜ë¥¼ êµ¬ì„±í–ˆìœ¼ë¯€ë¡œ ë³¸ê²©ì ìœ¼ë¡œ PyTorchë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.




```python
from torch.optim import AdamW

# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
optimizer = AdamW(model.parameters(), lr=5e-5)
```


```python
train(10, model, train_loader, optimizer, device)
```


    ================================================================
    TRAIN - EPOCH 1 | LOSS: 0.6821 ACC: 0.5530 F1: 0.4659
    ================================================================


    ================================================================
    TRAIN - EPOCH 2 | LOSS: 0.5740 ACC: 0.6825 F1: 0.6315
    ================================================================


    ================================================================
    TRAIN - EPOCH 3 | LOSS: 0.4387 ACC: 0.8055 F1: 0.7958
    ================================================================


    ================================================================
    TRAIN - EPOCH 4 | LOSS: 0.3261 ACC: 0.8760 F1: 0.8704
    ================================================================


    ================================================================
    TRAIN - EPOCH 5 | LOSS: 0.2744 ACC: 0.8895 F1: 0.8873
    ================================================================


    ================================================================
    TRAIN - EPOCH 6 | LOSS: 0.1774 ACC: 0.9405 F1: 0.9399
    ================================================================


    ================================================================
    TRAIN - EPOCH 7 | LOSS: 0.1397 ACC: 0.9545 F1: 0.9544
    ================================================================


    ================================================================
    TRAIN - EPOCH 8 | LOSS: 0.0979 ACC: 0.9695 F1: 0.9691
    ================================================================


    ================================================================
    TRAIN - EPOCH 9 | LOSS: 0.0688 ACC: 0.9750 F1: 0.9747
    ================================================================


    ================================================================
    TRAIN - EPOCH 10 | LOSS: 0.0436 ACC: 0.9845 F1: 0.9844
    ================================================================


ê²°ê³¼ë¥¼ ë¹¨ë¦¬ ë‚´ê¸° ìœ„í•´ random samplingëœ 2000ê°œì˜ ë°ì´í„°ë¡œ í•™ìŠµí•œ ê²ƒì´ê¸° ë•Œë¬¸ì— overfittingì´ ë°œìƒí•œ ê²ƒì„ ì§ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì´ìš©í•˜ë©´ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

model í‰ê°€ë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
* Trainerë¥¼ í™œìš©í•´ í•™ìŠµì„ ì§„í–‰í•œ ê²ƒê³¼ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ì–»ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
evaluate(model, test_loader, device)
```


    ================================================================
    EVAL | LOSS: 1.0961 ACC: 0.7610 F1: 0.7675
    ================================================================


ğŸ¤— huggingfaceì˜ pretrained modelì„ fine-tuningí•˜ëŠ” ë°©ë²•ì„ ê°„ë‹¨í•˜ê²Œ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.

ì´ë²ˆì—ëŠ” pretrained modelì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë¹ ë¥´ê²Œ ì•Œì•„ë³´ê¸° ìœ„í•´ Sequence Classfication ì¤‘ í•˜ë‚˜ì¸ Sentiment analysis(ê°ì • ë¶„ì„) taskë¥¼ ì§„í–‰í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë²ˆë¶€í„°ëŠ” ğŸ¤— huggingfaceì˜ How-To-Guidesë¥¼ ì‚´í´ë³´ë©´ì„œ NLP ë¶„ì•¼ì˜ downstream taskë¥¼ ì‹¤ìŠµí•´ë³´ëŠ”ì‹œê°„ì„ ì§„í–‰í•˜ë ¤ê³  í•©ë‹ˆë‹¤.

ê¸°íšŒê°€ ë˜ë©´ BERT, GPT ì™¸ì— ë‹¤ë¥¸ ëª¨ë¸ë„ í•œ ë²ˆ ë¦¬ë·°í•´ë³´ê² ìŠµë‹ˆë‹¤.
