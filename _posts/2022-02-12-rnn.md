---
title: "RNN"
excerpt: "RNN 구조와 내용"
use_math: true
toc: true
toc_sticky: true
categories:
    - deeplearning
tags:
    - python
    - pytorch
    - deeplearning
    - ai
sidebar:
    nav: sidebarTotal
---

## 1. Sequence Data, Time Series

### 1.1. Sequence Data, Time Series

-   Sequence Data(시퀀스 데이터): 데이터 내의 각 요소가 순서를 가지고 배열된 데이터. e.g., 문장

-   Time Series(시계열 데이터): 일정 시간 간격으로 배치된 데이터. e.g., 날씨, 주식

시퀀스 데이터, 시계열 데이터는 일반 데이터와는 다르게 '순서'에 종속적(dependency)이다.
지금 현재 데이터는 그 전 데이터들과 관련이 있기 때문에 현재 데이터를 해석하기 위해서는 그 주변 정보가 필요하다. 따라서 딥러닝을 이용해 시퀀스, 시계열 데이터를 학습할 때는 조금 특별한 형태의 신경망이 필요하다.

### 1.2. FFNN(Feed Forward Neural Network)의 단점

입력에서 출력까지 단방향으로 흐르는 신경망을 FFNN(Feed Forward Neural Network)이라고 부른다.
FFNN은 구조가 간단해서 구성하기 쉽다는 장점이 있지만 입력받은 데이터를 한 방향으로만 전달하고 끝나기 때문에 이전 정보가 필요한 시퀀스 데이터, 시계열 데이터에는 적합하지 않다.

## 2. RNN

'Recurrent'는 '되돌아오는, 반복되는'이라는 뜻으로 RNN을 직역하면 되돌아오는 신경망, 반복되는 신경망으로 번역할 수 있다. 신경망에 출력된 값이 다시 내부로 들어오는 경로가 존재하기 때문에 Recurrent라는 이름이 앞에 붙여졌다. RNN은 출력된 값이 다시 내부로 들어오기 때문에 시퀀스 데이터, 시계열 데이터같이 순서가 있는 자료형에 대해 적합하다.

### 2.1. 구조

RNN은 입력, 출력, 은닉 상태(hidden state)로 이루어져있다.

<center><img width='200' height='400' src="/assets/image/rnn/rnn1.png"></center>

RNN 구조에서 핵심이 되는 부분이 바로 이 은닉 상태이다.
은닉 상태는 RNN 내부에서 반복되면서 현재 시점까지의 정보를 다음 시점으로 넘겨주는 역할을 한다.
은닉 상태가 RNN 내부에서 반복되기 때문에 시퀀스 데이터, 시계열 데이터 같이 순서가 중요한 자료형을 다룰 수 있다.

### 2.2. 내부

<center><img width='400' height='200' src="/assets/image/rnn/rnn2.png"></center>

입력에서는 현재 시점의 입력 데이터를 받는다.

은닉 상태(hidden state)에서는 현재 시점 입력 데이터와 이전 시점의 은닉 상태를 활용하여 현재 시점의 은닉 상태를 만든 이후 다음 시점으로 보낸다.

출력에서는 만들어진 은닉 상태를 활용하여 현재 시점의 출력값을 반환한다.

이를 수식으로 살펴보면 다음과 같다.

먼저 RNN 내부에서 은닉 상태를 만들기 위해 2가지 가중치 행렬을 사용한다.

-   $W_x$: 입력 데이터를 위한 가중치 행렬
-   $W_h$: 이전 시점 은닉 상태를 위한 가중치 행렬

이 각각의 가중치 행렬과 $x_t$와 $h_{t-1}$을 이용하여 현재 시점 은닉 상태 $h_t$를 만든다.

-   $h_t = tanh(x_tW_x + h_{t-1}W_h + b)$

이렇게 만들어진 현재 시점 은닉 상태 $h_t$와 출력값을 만들기 위해 사용되는 가중치 행렬 $W_y$를 이용하여 현재 시점 출력값 $y_t$를 만든다.

-   $y_t = f(h_tW_h + b)$

$f$는 비선형 함수(activation function)로 상황에 따라 적절한 함수를 사용하면 된다.
RNN에 fully-connected layer와 같이 다른 층을 더 쌓는다면 ReLU 함수를 사용할 수도 있고 RNN만을 이용해 분류를 한다면 softmax를 이용할 수도 있다.

RNN 구조에 따라 $y_t$를 만들지 않고 $h_t$를 출력값으로 사용할 수도 있다.

<center><img width='200' height='400' src="/assets/image/rnn/rnn3.png"></center>

$h_t$만을 사용한다면 위와 같은 형태가 될 것이다.

## 3. RNN in PyTorch

PyTorch에서 multi-layer RNN을 지원한다.

multi-layer RNN은 RNN이 수직으로 여러 층 쌓인 구조이다. 가장 아래층에서 입력 데이터를 받아 RNN 학습을 한 후 다음 층으로 전달하는 형태를 지닌다. 즉, 첫번째 층을 제외하고 나머지 층은 입력값으로 이전 층의 출력값을 가지게 된다. 가장 마지막 층의 출력값이 multi-layer RNN의 최종 출력값이 된다.

### [torch.nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN)

**Parameters**

-   input_size: input x의 feature 수
-   hidden_size: hidden state h의 feature 수
-   num_layers: RNN 층 수
-   nonlinearlity: 사용할 비선형 함수 $tanh$('tanh') 또는 $ReLU$('relu') 사용가능

**inputs: input, h_0**

-   RNN은 입력으로 input과 hidden state에 사용할 가중치 행렬 $W_h$를 받는다.
-   input: (batch_size, sequence length, input_size) 또는 (sequence length,batch_size, input_size) shape을 지는 tensor
-   h_0: (num_layers, batch_size, hidden_size) shape을 지니는 tensor

**outputs: output, h_n**

-   출력으로 최종 출력값인 output과 각 batch의 마지막 hidden state를 반환한다.
