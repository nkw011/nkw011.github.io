---
title: "LSTM"
excerpt: "LSTM 구조와 내용"
use_math: true
toc: true
toc_sticky: true
categories:
    - deeplearning
tags:
    - python
    - pytorch
    - deeplearning
    - ai
sidebar:
    nav: sidebarTotal
---

## 1. RNN의 단점

기울기 소실(Vanishing Gradient)은 역전파시 기울기가 0에 가까워지면서 가중치가 갱신되지 않는 현상을 가리킨다.
RNN은 내부에서 $tanh$함수를 사용하기 때문에 입력 데이터가 길수록 기울기 소실이 발생할 가능성이 높다.

$tanh$함수를 미분해보자.

-   $tanhx = \frac{coshx}{sinhx} = \frac{e^x-e^{-x}}{e^x + e^{-x}}$
-   $\frac{dtanhx}{dx} = 1-tanh^2x$

```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-10,10,10000)
tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
dy = 1-tanh**2

fig, ax = plt.subplots()
ax.plot(x,dy)
ax.set_ylim([0,5])
```

    (0.0, 5.0)

<img src='/assets/image/lstm/lstm-tanh.png'>

$\frac{dtanh}{dx}$의 최댓값은 1이고 $x$가 0으로부터 멀어질수록 작아진다.
따라서 RNN 역전파시 $tanh$를 거칠 때마다 값은 계속해서 작아지게된다.
이런 현상으로 인해 RNN에서 기울기 소실이 발생하게 된다.

RNN내에서 $tanh$ 대신 $ReLU$를 사용하게 되면 기울기 소실이 발생할 가능성이 어느정도 줄어들게 된다.

앞선 글에서 언급했듯이 [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) RNN에서는 $tanh$와 $ReLU$중 선택해서 사용할 수 있다.

## 2. LSTM

LSTM(Long Short-Term Memory)은 RNN의 이러한 단점을 보완한 모델이다.
LSTM은 기존 RNN에서 cell state와 gate를 추가하여 보다 복잡하지만 긴 입력 데이터를 처리하는데 좋은 성능을 보인다.

### 2.1. 구조

<img width='370' height='321' src='/assets/image/lstm/lstm1.png'>

LSTM은 시점 $t$에서 세가지 정보를 입력받는다.

-   $x_t$: 현재 시점 입력 데이터
-   $c_{t-1}$: 이전 시점 cell state
-   $h_{t-1}$: 이전 시점 hidden state

그리고 다음 시점을 향해 현재 시점의 $c_t$와 $h_t$를 넘겨주고 $h_t$를 출력하는 형태를 지닌다.

'cell state'는 기억셀이라고도 하며 LSTM에서 사용하는 기억 저장소이다. cell state는 내부에서만 반복되는 형태를 지니고 있기 때문에 밖으로 출력되지 않는다.

### 2.2. 내부

LSTM은 내부에 gate를 가지고 있다. gate에서는 데이터가 얼마나 사용될지 결정하는 역할을 한다.
sigmoid함수를 이용해 gate로 들어오는 입력 정보를 0~1사이의 값으로 반환하는데 값이 0에 가까우면 gate의 반환값과 곱해지는 데이터를 거의 사용하지 않는 다는 뜻이 되고 1에 가까우면 gate의 반환값과 곱해지는 데이터를 대부분 사용한다는 뜻이 된다.
아래부터 sigmoid 함수는 $\sigma$로 표시할 것이다.

LSTM은 다음의 과정을 통해 학습을 진행하게된다.

#### (1) forget gate

-   이전 시점 cell state 정보를 얼마나 반영할 지 결정하는 역할
-   $x_t$와 $h_{t-1}$을 이용하여 결정한다.
-   $f = \sigma(x_tW_x^{(f)} + h_{t-1}W_h^{(f)} + b^{(f)})$
-   $f$와 $c_{t-1}$을 곱하여 $c_{t-1}$의 정보를 얼마나 반영할 지(삭제할 지) 결정한다.

<img width='370' height='229' src='/assets/image/lstm/lstm2.png'>

#### (2) 새로운 정보 생산

-   $x_t$와 $h_{t-1}$을 이용하여 현재 시점의 데이터를 반영할 정보를 생산한다.
-   $g = tanh(x_tW_x^{(g)} + h_{t-1}W_h^{(g)} + b^{(g)})$

<img width='370' height='218' src='/assets/image/lstm/lstm3.png'>

#### (3) input gate

-   새롭게 생산한 정보를 얼마나 반영할 지 결정하는 역할
-   $x_t$와 $h_{t-1}$을 이용하여 결정한다.
-   $i = \sigma(x_tW_x^{(i)} + h_{t-1}W_h^{(i)} + b^{(i)})$

<img width='370' height='209' src='/assets/image/lstm/lstm4.png'>

#### (4) 현재 시점의 cell state 생산

forget gate를 통과한 $c_{t-1}$과 $g$와 $i$를 곱하여 새롭게 cell state를 만든다.

<img width='370' height='218'src='/assets/image/lstm/lstm5.png'>

#### (5) 현재 시점의 hidden state 후보 만들기

-   만들어진 cell state를 활용해 $h_t$를 생산한다.
-   $tanh(c_t)$

#### (6) output gate

-   tanh(c_t)를 얼마나 내보낼 지 결정하는 역할
-   $x_t$와 $h_{t-1}$을 이용하여 결정한다.
-   $o = \sigma(x_tW_x^{(o)} + h_{t-1}W_h^{(o)} + b^{(o)})$

<img width='370' height='222' src='/assets/image/lstm/lstm6.png'>

#### (7) 현재 시점의 hidden state 생산

-   $h_t = tanh(c_t) \odot o$
-   이렇게 만든 $h_t$와 $c_t$를 다음으로 전달한다.

## 3. LSTM in PyTorch

RNN과 마찬가지로 multi-layer LSTM을 지원한다.

### [torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)

**Parameters**

-   input_size: input x의 feature 수
-   hidden_size: hidden state h의 feature 수
-   num_layers: 사용할 LSTM 층 수

**inputs: input, (h_0, c_0)**

-   input: (sequece length, batch, input_size) 또는 (batch, sequence length, input_size) shape의 tensor
-   h_0: (num_layers,batch,hidden_size) shape의 tensor
-   c_0: (num_layers,batch,hidden_size) shape의 tensor

**outputs: output, (h_n, c_n)**

-   output: 각 batch에서 가장 마지막 층의 출력값
-   h_n: 각 batch에서 final hidden state 반환
-   c_n: 각 batch에서 final cell state 반환
