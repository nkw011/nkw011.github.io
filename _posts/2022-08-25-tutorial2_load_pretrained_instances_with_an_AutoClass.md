---
title: "[Huggingface ğŸ¤— Transformers Tutorial] 2. Load pretrained instances with an AutoClass"
excerpt: "ğŸ¤— Transformersì˜ AutoClassì˜ ì¢…ë¥˜ì™€ í™œìš©ë°©ë²•ì— ëŒ€í•´ ë°°ì›Œë´…ë‹ˆë‹¤."
use_math: true
toc: true
toc_sticky: true
categories:
    - nlp
tags:
    - huggingface
    - transformers
sidebar:
    nav: sidebarTotal
---

â€» ì´ ê¸€ì˜ ì›ë¬¸ì€ [ì´ ê³³](https://huggingface.co/docs/transformers/autoclass_tutorial)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â€» ëª¨ë“  ê¸€ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©° ìƒˆë¡­ê²Œ êµ¬ì„±í•œ ë‚´ìš©ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ğŸ¤— TransformersëŠ” [Model Hub](https://huggingface.co/models)ì— ë“±ë¡ëœ ìˆ˜ë§ì€ pretrained modelì„ ê°„í¸í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ [AutoClass](https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/auto#auto-classes)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
* from_pretrained ë©”ì†Œë“œë¥¼ ì´ìš©í•´ì„œ ì†ì‰½ê²Œ modelê³¼ tokenizerì„ loadí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

ì´ë²ˆ ì‹œê°„ì—ëŠ” AutoClassë¥¼ ì´ìš©í•´ì„œ tokenizer, modelì„ loadí•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.


```python
!pip install transformers
```

## AutoTokenizer

AutoTokenizerë¥¼ ì´ìš©í•´ì„œ í•™ìŠµì— í•„ìš”í•œ tokenizerë¥¼ ì†ì‰½ê²Œ loadí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

AutoTokenizer.from_pretrained()ë¥¼ ì´ìš©í•´ì„œ [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)ì˜ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.


```python
MODEL_NAME = 'bert-base-multilingual-cased'
```


```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

bert-base-multilingual-cased tokenizerë¥¼ ì‚¬ìš©í•˜ê¸°ìœ„í•´ í•„ìš”í•œ config, vocab ë“±ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.


```python
tokenizer
```




    PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})



tokenizer.vocabì„ ì´ìš©í•˜ì—¬ vocabì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
tokenizer.vocab

# ê²°ê³¼ëŠ” ìƒëµí•©ë‹ˆë‹¤..
```



tokenizerë¥¼ ì´ìš©í•´ì„œ í•œêµ­ì–´ ë¬¸ì¥ì„ tokenizationì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

1. tokenizerë¥¼ callí•˜ëŠ” ë°©ì‹
2. tokenize()
3. encode()


ì²«ë²ˆì§¸ë¡œ, tokenizerë¥¼ callí•˜ëŠ” ë°©ì‹ì„ ì´ìš©í•´ tokenizationì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

tokenizerë¥¼ callí•˜ëŠ” ë°©ì‹ì„ ì´ìš©í•˜ë©´ ì£¼ì–´ì§„ textë¥¼ tokenizationí•œ ë’¤ model ì…ë ¥ì— í•„ìš”í•œ ëª¨ë“  ìš”ì†Œë¥¼ ë°˜í™˜í•´ì¤ë‹ˆë‹¤.


```python
text = 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'
```


```python
# 1. tokenizerë¥¼ callí•˜ëŠ” ë°©ì‹

tokenized_text = tokenizer(text)

print(tokenized_text)
```

    {'input_ids': [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}


input_ids, token_type_ids, attention_maskê°€ ë°˜í™˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìš´ë¡œë“œ ë°›ì€ tokenizerê°€ BERT ëª¨ë¸ì„ ìœ„í•œ tokenizerì´ê¸° ë•Œë¬¸ì— ğŸ¤— Transformersì— ì •ì˜ëœ BERT architectureê°€ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í˜•íƒœì¸ input_ids, token_type_ids, attention_maskë¡œ ë§ì¶”ì–´ ë°˜í™˜í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê°„ë‹¨í•˜ê²Œ ê°ê°ì˜ ì—­í• ì— ëŒ€í•´ ì„¤ëª…í•˜ë©´,
* input_ids: tokenizedëœ textì— ì• ë’¤ë¡œ special tokenì„ ì¶”ê°€í•œë’¤ idê°’ìœ¼ë¡œ ë³€í˜•í•œ ê°’ì„ ëœ»í•©ë‹ˆë‹¤.
* token_type_ids: BERTì˜ segment embeddingì„ ëœ»í•©ë‹ˆë‹¤. í˜„ì¬ ë¬¸ì¥ì´ 1ê°œê°€ ë“¤ì–´ì™”ê¸° ë•Œë¬¸ì— token_type_idsê°€ ëª¨ë‘ 0ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* attention_mask: pad tokenì€ 0ìœ¼ë¡œ ë‚˜ë¨¸ì§€ tokenë“¤ì€ 1ì„ ê°€ì§‘ë‹ˆë‹¤. attention ê³„ì‚°ì„ ìœ„í•´ ë¶ˆí•„ìš”í•œ ê°’ë“¤ì„ masking ì²˜ë¦¬í•œë‹¤ê³  ë³´ë©´ ë©ë‹ˆë‹¤.

ë°˜í™˜ëœ ê°’ì„ ê·¸ëŒ€ë¡œ modelì˜ ì…ë ¥ì— ë„£ì–´ì£¼ë©´ ë©ë‹ˆë‹¤.

ë‘ë²ˆì§¸ë¡œ, tokenize()ë¥¼ ì´ìš©í•˜ì—¬ tokenizationí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

tokenize() ë©”ì†Œë“œëŠ” ì£¼ì–´ì§„ textë¥¼ ë‹¨ìˆœíˆ tokenizationë§Œ í•´ì¤ë‹ˆë‹¤.


```python
print(tokenizer.tokenize(text))
```

    ['ì´', '##ìˆœ', '##ì‹ ', '##ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.']


ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. â†’ ['ì´', '##ìˆœ', '##ì‹ ', '##ì€', 'ì¡°ì„ ', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.']ë¡œ tokenizationëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì„¸ë²ˆì§¸ë¡œ, encode()ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

encode()ëŠ” tokenizedëœ textì— ì• ë’¤ë¡œ special tokenì„ ì¶”ê°€í•œë’¤ idê°’ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
input_idsì™€ ë™ì¼í•©ë‹ˆë‹¤.


```python
print(tokenizer.encode(text))
```

    [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102]


input_idsì™€, encode()ì˜ ë°˜í™˜ê°’ì„ ëª¨ë‘ decode()ë¥¼ ì´ìš©í•˜ì—¬ ì›ë˜ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ì‹œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì• ë’¤ì— [CLS], [SEP] tokenì´ ë¶™ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
tokenized_text1 = tokenizer(text)
print(tokenized_text1['input_ids'])
print(tokenizer.decode(tokenized_text1['input_ids']))

tokenizer_text2 = tokenizer.encode(text)
print(tokenizer_text2)
print(tokenizer.decode(tokenizer_text2))
```

    [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102]
    [CLS] ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]
    [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119, 102]
    [CLS] ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. [SEP]


## AutoModel

AutoModelì„ ì´ìš©í•˜ì—¬ base modelì„ loadí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* base modelì´ë¼ í•¨ì€ taskë¥¼ ìœ„í•œ classifierë“±ì´ ë¶€ì°©ë˜ì§€ ì•Šì€ vanilla í˜•íƒœë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.
* AutoModelì„ ì´ìš©í•´ taskë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ë³„ë„ì˜ classifierë“±ì„ ë¶€ì°©í•´ì•¼í•©ë‹ˆë‹¤.

ğŸ¤— Transformersì—ì„œëŠ” ë‹¤ì–‘í•œ taskë“¤ì— ì í•©í•œ model archtectureë¥¼ ì´ë¯¸ AutoClass í˜•íƒœë¡œ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ AutoModelForMaskedLMì€ masked langunage modelingì„ ìœ„í•œ AutoClassì…ë‹ˆë‹¤.
* AutoClass ëª©ë¡ì€ [ì—¬ê¸°](https://huggingface.co/docs/transformers/model_doc/auto#auto-classes)ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* AutoModelForQuestionAnswering
* AutoModelForSequenceClassification
* AutoModelForTokenClassification


'bert-base-multilingual-cased'ì˜ base modelê³¼ Token Classification modelì„ loadí•˜ì—¬ í˜•íƒœê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.



```python
MODEL_NAME = 'bert-base-multilingual-cased'
```


```python
from transformers import AutoModel, AutoModelForTokenClassification

base_model = AutoModel.from_pretrained(MODEL_NAME)
token_class_model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)
```


```python
base_model
```




    BertModel() # ë‚´ìš© ì¼ë¶€ë¥¼ ìƒëµí–ˆìŠµë‹ˆë‹¤.




```python
token_class_model
```




    BertForTokenClassification(
      (bert): BertModel() # ë‚´ìš© ì¼ë¶€ë¥¼ ìƒëµí–ˆìŠµë‹ˆë‹¤.
      (dropout): Dropout(p=0.1, inplace=False)
      (classifier): Linear(in_features=768, out_features=2, bias=True)
    ) 



Token Classification modelì— classifierê°€ ì¶”ê°€ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
