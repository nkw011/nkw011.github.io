---
title: "[Huggingface ğŸ¤— Transformers Tutorial] 3. Preprocess"
excerpt: "ğŸ¤— Transformersì˜ AutoTokenizerë¥¼ í™œìš©í•˜ì—¬ textë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë´…ë‹ˆë‹¤."
use_math: true
toc: true
toc_sticky: true
categories:
    - nlp
tags:
    - huggingface
    - transformers
sidebar:
    nav: sidebarTotal
---

â€» ì´ ê¸€ì˜ ì›ë¬¸ì€ [ì´ ê³³](https://huggingface.co/docs/transformers/preprocessing)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â€» ëª¨ë“  ê¸€ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©° ìƒˆë¡­ê²Œ êµ¬ì„±í•œ ë‚´ìš©ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

tokenizerë¥¼ ì´ìš©í•´ì„œ ì£¼ì–´ì§„ textë¥¼ modelì´ ìš”êµ¬í•˜ëŠ” í˜•ì‹ì— ë§ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ê°„ë‹¨í•˜ê²Œ ìµí˜€ë´…ë‹ˆë‹¤.


```python
!pip install transformers
```

ë¨¼ì € ì „ì²˜ë¦¬í•  ë¬¸ì¥ 3ê°œë¥¼ ì •ì˜í™ë‚˜ë‹¤.
* í•´ë‹¹ ë¬¸ì¥ë“¤ì€ ëª¨ë‘ [ìœ„í‚¤ë°±ê³¼-ì´ìˆœì‹ ](https://ko.wikipedia.org/wiki/%EC%9D%B4%EC%88%9C%EC%8B%A0)ì—ì„œ ì¼ë¶€ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.


```python
texts = ['ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ì—ˆë‹¤.', 'ë³¸ê´€ì€ ë•ìˆ˜, ìëŠ” ì—¬í•´, ì‹œí˜¸ëŠ” ì¶©ë¬´ì˜€ìœ¼ë©°, í•œì„± ì¶œì‹ ì´ì—ˆë‹¤.', 'ì˜¥í¬í•´ì „ì€ ì´ìˆœì‹ ì˜ ì²« ìŠ¹ì „ì„ ì•Œë¦¬ê²Œ ëœ í•´ì „ì´ë‹¤.']
```

## Tokenize

tokenizationì— í•„ìš”í•œ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ['bert-base-multilingual-cased'](https://huggingface.co/bert-base-multilingual-cased)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.


```python
MODEL_NAME = 'bert-base-multilingual-cased'
```


```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```


tokenizerë¥¼ ì´ìš©í•´ tokenizationí•˜ëŠ” ë°©ë²•ì€ 3ê°€ì§€ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

1. tokenizerë¥¼ callí•˜ëŠ” ë°©ì‹
2. tokenize() ë©”ì†Œë“œ
3. encode() ë©”ì†Œë“œ

2,3ë²ˆ ë°©ì‹ì€ tokenizationì„ í•˜ì§€ë§Œ modelì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í˜•íƒœë¡œ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ tokenizationê³¼ encodingë§Œ í•´ì¤„ ë¿ì…ë‹ˆë‹¤.

1ë²ˆ ë°©ì‹ì„ ì´ìš©í•´ modelì´ ìš”êµ¬í•˜ëŠ” ëª¨ë“  ì…ë ¥ ìš”ì†Œë¥¼ ë°˜í™˜í•˜ê² ìŠµë‹ˆë‹¤.


```python
encoded_input = tokenizer(texts[0])

print(encoded_input)
```

    {'input_ids': [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 58926, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}


í•œ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë¬¸ì¥ë“¤ì„ ë™ì‹œì— tokenizationí•˜ê¸° ìœ„í•´ì„œëŠ” ì…ë ¥ìœ¼ë¡œ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì£¼ë©´ ë©ë‹ˆë‹¤.


```python
encoded_inputs = tokenizer(texts)

for key in encoded_inputs.keys():
    print(f"{key}(ê¸¸ì´: {len(encoded_inputs[key])}): {encoded_inputs[key]}")
```

    input_ids(ê¸¸ì´: 3): [[101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 58926, 119, 102], [101, 9358, 101958, 9075, 15891, 113, 3789, 4867, 114, 117, 9651, 11018, 9565, 14523, 113, 4886, 7247, 114, 117, 9485, 100543, 9770, 32537, 113, 3803, 4794, 114, 9573, 24098, 117, 9954, 17138, 9768, 25387, 58926, 119, 102], [101, 9581, 55530, 14523, 16617, 10892, 9638, 119064, 87143, 9750, 9484, 54918, 9524, 12692, 14153, 9099, 9960, 16617, 11925, 119, 102]]
    token_type_ids(ê¸¸ì´: 3): [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
    attention_mask(ê¸¸ì´: 3): [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]


textëŠ” ì´ 3ê°œ ì˜€ìŠµë‹ˆë‹¤. input_ids, token_type_ids, atttention_mask ëª¨ë‘ 3ê°œê°€ ë§Œë“¤ì–´ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Pad

ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” batchë¡œ ë“¤ì–´ì˜¤ëŠ” ëª¨ë“  sequenceì˜ ê¸¸ì´ê°€ ë™ì¼í•´ì•¼í•©ë‹ˆë‹¤. padding tokenì„ ì¶”ê°€í•˜ì—¬ sequenceì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
# pad token id
print(tokenizer.vocab['[PAD]'])
```

    0



```python
encoded_inputs = tokenizer(texts, padding=True)

for key in encoded_inputs.keys():
    print(key,":")
    for data in encoded_inputs[key]:
        print(data)
```

    input_ids :
    [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 58926, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [101, 9358, 101958, 9075, 15891, 113, 3789, 4867, 114, 117, 9651, 11018, 9565, 14523, 113, 4886, 7247, 114, 117, 9485, 100543, 9770, 32537, 113, 3803, 4794, 114, 9573, 24098, 117, 9954, 17138, 9768, 25387, 58926, 119, 102]
    [101, 9581, 55530, 14523, 16617, 10892, 9638, 119064, 87143, 9750, 9484, 54918, 9524, 12692, 14153, 9099, 9960, 16617, 11925, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    token_type_ids :
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    attention_mask :
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


ê°€ì¥ ê¸¸ì´ê°€ ê¸´ 2ë²ˆì§¸ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ 1, 3ë²ˆì§¸ ë¬¸ì¥ì— padding tokenì´ ì¶”ê°€ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Truncation

modelì´ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” sequenceì˜ ê¸¸ì´ëŠ” ì •í•´ì ¸ìˆìŠµë‹ˆë‹¤. ë„ˆë¬´ ê¸´ ê¸¸ì´ëŠ” ë‹¤ë£¨ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

truncation=True parameterë¥¼ ì´ìš©í•˜ì—¬ maximum lengthë³´ë‹¤ ê¸¸ ê²½ìš° ì´ˆê³¼í•œ ë§Œí¼ ê¸¸ì´ë¥¼ ì˜ë¼ëƒ…ë‹ˆë‹¤.

ë³´í†µ max_length parameterë„ ê°™ì´ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤. max_lengthë¥¼ ì´ìš©í•˜ì—¬ maximum lengthë¥¼ í†µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
print(tokenizer(texts, padding=True, truncation=True))
```

    {'input_ids': [[101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 58926, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9358, 101958, 9075, 15891, 113, 3789, 4867, 114, 117, 9651, 11018, 9565, 14523, 113, 4886, 7247, 114, 117, 9485, 100543, 9770, 32537, 113, 3803, 4794, 114, 9573, 24098, 117, 9954, 17138, 9768, 25387, 58926, 119, 102], [101, 9581, 55530, 14523, 16617, 10892, 9638, 119064, 87143, 9750, 9484, 54918, 9524, 12692, 14153, 9099, 9960, 16617, 11925, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}


ì£¼ì–´ì§„ textëŠ” BERTê°€ ë‹¤ë£° ìˆ˜ ìˆëŠ” maximum lengthë³´ë‹¤ lengthê°€ ì‘ê¸° ë•Œë¬¸ì— truncationì´ ë˜ì§€ ì•Šì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
encoded_inputs = tokenizer(texts, max_length=12)

for key in encoded_inputs.keys():
    print(key,":")
    for data in encoded_inputs[key]:
        print(data)
```

    input_ids :
    [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 58926, 102]
    [101, 9358, 101958, 9075, 15891, 113, 3789, 4867, 114, 117, 9651, 102]
    [101, 9581, 55530, 14523, 16617, 10892, 9638, 119064, 87143, 9750, 9484, 102]
    token_type_ids :
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    attention_mask :
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]


max_lengthë¥¼ ì´ìš©í•˜ë©´ maximum lengthë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Build tensors

return_tensors parameterë¥¼ ì´ìš©í•˜ì—¬ ì›í•˜ëŠ” í˜•íƒœì˜ tensorë¡œ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* return_tensors='pt': PyTorch
* return_tensors='tf': TensorFlow


```python
encoded_inputs = tokenizer(texts, padding=True, truncation=True, max_length=15, return_tensors='pt')

print(encoded_inputs)
```

    {'input_ids': tensor([[   101,   9638, 119064,  25387,  10892,  59906,   9694,  46874,   9294,
              25387,  58926,    119,    102,      0,      0],
            [   101,   9358, 101958,   9075,  15891,    113,   3789,   4867,    114,
                117,   9651,  11018,   9565,  14523,    102],
            [   101,   9581,  55530,  14523,  16617,  10892,   9638, 119064,  87143,
               9750,   9484,  54918,   9524,  12692,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}

