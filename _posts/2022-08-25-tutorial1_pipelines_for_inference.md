---
title: "[Huggingface ğŸ¤— Transformers Tutorial] 1. Pipelines for inference"
excerpt: "ğŸ¤— Transformersì˜ pipeline() ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ì—¬ ìì—°ì–´ì²˜ë¦¬ taskë¥¼ ê°„ë‹¨í•˜ê²Œ ìˆ˜í–‰í•©ë‹ˆë‹¤."
use_math: true
toc: true
toc_sticky: true
categories:
    - nlp
tags:
    - huggingface
    - transformers
sidebar:
    nav: sidebarTotal
---

â€» ì´ ê¸€ì˜ ì›ë¬¸ì€ [ì´ ê³³](https://huggingface.co/docs/transformers/pipeline_tutorial)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
â€» ëª¨ë“  ê¸€ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©° ìƒˆë¡­ê²Œ êµ¬ì„±í•œ ë‚´ìš©ë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

[pipeline()](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/pipelines#pipelines) í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì •í™•íˆ ëª¨ë¥´ë”ë¼ë„ pretrained ëª¨ë¸ì„ ì´ìš©í•´ ìì—°ì–´ì²˜ë¦¬ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* [pipeline()ì„ ì´ìš©í•´ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” task](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/pipelines#transformers.pipeline.task)
    * text-classification
    * text-generation
    * token-classification
    * fill-mask

[Model Hub](https://huggingface.co/models)ì—ì„œ pretrained modelë“¤ì„ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ë³„ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” taskê°€ ëª¨ë‘ ë‹¤ë¥´ë¯€ë¡œ taskì— ì í•©í•œ ëª¨ë¸ì„ ì°¾ì•„ì•¼í•©ë‹ˆë‹¤.

## pipeline()ì„ ì´ìš©í•´ fill-mask task ìˆ˜í–‰í•˜ê¸°

taskì— ì í•©í•œ modelì„ ì°¾ì•˜ë‹¤ë©´ AutoModel, AutoTokenizer í´ë˜ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ modelê³¼ modelì— ì‚¬ìš©ë˜ëŠ” tokenizerë¥¼ ê°„ë‹¨í•˜ê²Œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* AutoClassì— ê´€í•´ì„œëŠ” ë‹¤ìŒ ê¸€ì—ì„œ ë‹¤ë£¹ë‹ˆë‹¤.
* ì´ë²ˆì—ëŠ” fill-maskë¥¼ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— AutoModelForMaskedLM í´ë˜ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (AutoModelì„ ì´ìš©í•  ê²½ìš° ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.)


```python
!pip install transformers
```

í•œêµ­ì–´ fill-mask taskë¥¼ ìˆ˜í–‰í•˜ê¸°ìœ„í•´ BERT pretrained ëª¨ë¸ ì¤‘ì—ì„œ [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
* ë‹¤ì–‘í•œ ì–¸ì–´ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” multilingual modelì…ë‹ˆë‹¤.

from_pretrained()ì— model ì´ë¦„ì„ ë„£ìœ¼ë©´ ì†ì‰½ê²Œ pretrained model, tokenizerë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

* ì¼ë°˜ì ìœ¼ë¡œ modelì— ì‚¬ìš©ë˜ëŠ” configuration, tokenizerê°€ ëª¨ë‘ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ë ¤ëŠ” modelì— ì í•©í•œ configuration, tokenizerë¥¼ ë¶ˆëŸ¬ì™€ì•¼í•©ë‹ˆë‹¤.


```python
from transformers import AutoModelForMaskedLM, AutoTokenizer

MODEL_NAME = 'bert-base-multilingual-cased'
model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

ë¨¼ì € tokenizerê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
* ì›ë¬¸: ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.
* mask: ì´ìˆœì‹ ì€ [MASK] ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.

fill-mask taskë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ textë‚´ì— [MASK] special tokenì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼í•©ë‹ˆë‹¤.


```python
text = "ì´ìˆœì‹ ì€ [MASK] ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."

tokenizer.tokenize(text)
```




    ['ì´', '##ìˆœ', '##ì‹ ', '##ì€', '[MASK]', 'ì¤‘', '##ê¸°ì˜', 'ë¬´', '##ì‹ ', '##ì´ë‹¤', '.']



BERTëŠ” WordPiece ë°©ì‹ì˜ tokenizationì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ##ì´ë¼ëŠ” íŠ¹ë³„í•œ prefixê°€ ë¶™ì–´ìˆëŠ” tokenë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ##ì€ í•´ë‹¹ tokenì´ ì›ë˜ëŠ” ì• tokenê³¼ ë¶™ì–´ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. e.g.) ì´ìˆœì‹  â†’ ì´, ##ìˆœ, ##ì‹ 

pipeline()ì„ ì´ìš©í•´ í•œêµ­ì–´ fill-mask taskë¥¼ ìˆ˜í–‰í•˜ê¸°ìœ„í•œ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.


```python
from transformers import pipeline

kor_mask_fill = pipeline(task='fill-mask', model=model, tokenizer=tokenizer)
```

kor_mask_fill í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ fill-mask taskë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.


```python
text = "ì´ìˆœì‹ ì€ [MASK] ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."

kor_mask_fill("ì´ìˆœì‹ ì€ [MASK] ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.")
```




    [{'score': 0.874712347984314,
      'token': 59906,
      'token_str': 'ì¡°ì„ ',
      'sequence': 'ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'},
     {'score': 0.0643644854426384,
      'token': 9751,
      'token_str': 'ì²­',
      'sequence': 'ì´ìˆœì‹ ì€ ì²­ ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'},
     {'score': 0.010954903438687325,
      'token': 9665,
      'token_str': 'ì „',
      'sequence': 'ì´ìˆœì‹ ì€ ì „ ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'},
     {'score': 0.004647187888622284,
      'token': 22200,
      'token_str': '##ì¢…',
      'sequence': 'ì´ìˆœì‹ ì€ì¢… ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'},
     {'score': 0.0036106701008975506,
      'token': 12310,
      'token_str': '##ê¸°',
      'sequence': 'ì´ìˆœì‹ ì€ê¸° ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤.'}]



[MASK] ìë¦¬ì— ë“¤ì–´ê°ˆ tokenë“¤ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
* score: ì ìˆ˜
* token: token id
* token_str: token text
